# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆå¸•é‡‘æ£®ç—…è¯­éŸ³ç‰¹å¾åˆ†æç³»ç»Ÿ + å¤šæ¨¡æ€äº¤å‰æ³¨æ„åŠ›èåˆ + ä¸ç¡®å®šæ€§åŠ æƒå¤šä»»åŠ¡å­¦ä¹ 
Enhanced Parkinson's Disease Voice Analysis with Multi-Modal Cross-Attention Fusion
and Uncertainty-Aware Multi-Task Learning

åˆ›æ–°ç‚¹:
1. å¤šæ¨¡æ€äº¤å‰æ³¨æ„åŠ›èåˆæœºåˆ¶ (Multi-Modal Cross-Attention Fusion, MM-CAF)
2. ä¸ç¡®å®šæ€§åŠ æƒçš„å¤šä»»åŠ¡æŸå¤± (Uncertainty-Aware Multi-Task Loss Strategy)
"""

# --- æ ¸å¿ƒåº“å¯¼å…¥ ---
import os
import traceback
from pathlib import Path
from typing import List

from tqdm import tqdm
# --- æ•°æ®å¤„ç†ä¸æœºå™¨å­¦ä¹  ---
import numpy as np
import pandas as pd
from scipy import signal
from scipy.stats import kurtosis, skew
from sklearn.model_selection import StratifiedKFold, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import (accuracy_score, balanced_accuracy_score, roc_auc_score)

# --- æ·±åº¦å­¦ä¹  (TensorFlow/Keras) ---
import tensorflow as tf
from tensorflow.keras import layers, Model
from tensorflow.keras.losses import SparseCategoricalCrossentropy

# --- éŸ³é¢‘å¤„ç† ---
import librosa
import librosa.display

import pickle
from datetime import datetime
from pathlib import Path

# --- å¯è§†åŒ– ---
from P4_vis import SCIPaperVisualization

# --- å¯é€‰åº“å¯¼å…¥ä¸å¯ç”¨æ€§æ£€æŸ¥ ---
try:
    from tensorflow_addons.optimizers import AdamW

    print("Using AdamW from tensorflow_addons.")
except ImportError:
    print("Warning: tensorflow_addons not found. Falling back to tf.keras.optimizers.AdamW.")
    from tensorflow.keras.optimizers import AdamW

try:
    from imblearn.over_sampling import SMOTE

    SMOTE_AVAILABLE = True
except ImportError:
    SMOTE_AVAILABLE = False
    print("Warning: imbalanced-learn not found. SMOTE will be disabled.")

try:
    import shap

    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False
    print("Warning: SHAP not found. Model interpretability will be limited.")

try:
    import torch
    import torch.nn as nn
    from transformers import Wav2Vec2Processor, Wav2Vec2Model,trainer

    TORCH_AVAILABLE = True
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"PyTorch available. Using device: {DEVICE}")
except ImportError:
    TORCH_AVAILABLE = False
    print("Warning: PyTorch/Transformers not found. Wav2Vec features will be disabled.")

# --- é…ç½®å‚æ•° ---
CONFIG = {
    'DATA_PATH': r'D:\data',  # â† åªåˆ°æ ¹ç›®å½•
    'OUTPUT_PATH': r'./results',
    'TARGET_COLUMN': 'status',
    'SAMPLE_RATE': 16000,
    'RANDOM_STATE': 42,
    'BATCH_SIZE': 16,  # å®¶ç”¨ GPU å¯å†è°ƒå°
    'EPOCHS': 60,  # å°æ•°æ®ä¸ç”¨ 100
    'LEARNING_RATE': 3e-4,
    'WEIGHT_DECAY': 1e-4,
    'PATIENCE': 10,
    'SCALE_DATA': True,
    'USE_SMOTE': True,
    'SMOTE_K_NEIGHBORS': 3,  # æ ·æœ¬å°‘æ—¶ k è¦å°äºæœ€å°ç±»
    'N_SPLITS': 2,  # 2â†’5ï¼Œæ›´ç¨³å¥
    'MAX_FILES_PER_CLASS': None,  # æƒ³å¿«é€Ÿè°ƒè¯•å¯è®¾ 50
    'USE_CONTRASTIVE_LOSS': True,
    'CONTRASTIVE_LOSS_WEIGHT': 0.4,
    'CONTRASTIVE_TEMPERATURE': 0.1,
    'INITIAL_TEMP_SCALE': 10.0,
    'TRAINABLE_TEMP': True,
    'RUN_ABLATION_STUDY': False,  # å…ˆå…³ï¼Œè·‘é€šå†å¼€
    'ABLATION_CONFIGS': [
        {'EXP_NAME': 'MM_CAF_Uncertainty', 'ENCODER_TYPE': 'mm_caf'},
    ],
    'ENCODER_TYPE': 'mm_caf',
    'EMBEDDING_DIM': 256,  # 768â†’256ï¼Œå°æ•°æ®é™ç»´
    'USE_CLINICAL_CENTER_LOSS': False,  # æ²¡æœ‰ UPDRS æ–‡ä»¶å…ˆå…³
    'CLINICAL_LOSS_WEIGHT': 0.0,
    'USE_UNCERTAINTY_WEIGHTING': True,
    'HANDCRAFTED_DIM': 40,
    'DEEP_FEATURE_DIM': 768,
    # æ–°å¢çš„ä¼˜åŒ–é…ç½®
    'FAST_MODE': True,  # å¯ç”¨å¿«é€Ÿæ¨¡å¼
    'AUDIO_SEGMENT_LENGTH': 3,  # éŸ³é¢‘æ®µé•¿åº¦ï¼ˆç§’ï¼‰
    'MAX_FILES_PER_CLASS': None,  # æ¯ç±»æœ€å¤§æ–‡ä»¶æ•°ï¼ˆNoneè¡¨ç¤ºä¸é™åˆ¶ï¼‰
    'SKIP_WAV2VEC': False,  # æ˜¯å¦è·³è¿‡Wav2Vecç‰¹å¾æå–

'USE_CACHE': True,  # æ˜¯å¦å¯ç”¨ç¼“å­˜åŠŸèƒ½
    'CACHE_DIR': 'cache',  # ç¼“å­˜ç›®å½•å
    'AUTO_USE_CACHE': False,  # æ˜¯å¦è‡ªåŠ¨ä½¿ç”¨ç¼“å­˜ï¼ˆä¸è¯¢é—®ç”¨æˆ·ï¼‰
}



# --- å…¨å±€è®¾ç½® ---
tf.random.set_seed(CONFIG['RANDOM_STATE'])
np.random.seed(CONFIG['RANDOM_STATE'])
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'


# -------------- æ–°å¢æ‰¹é‡ GPU æ¥å£ --------------
def extract_batch(self, wav_list: List[np.ndarray]) -> np.ndarray:
    """ä¸€æ¬¡æŠŠ 32Ã—30 s è¯­éŸ³å–‚ç»™ GPUï¼Œè¿”å› 32Ã—768 å‘é‡"""
    # ç»Ÿä¸€é•¿åº¦ï¼ˆ30 sï¼‰é¿å…åŠ¨æ€ padding æ‹–æ…¢ CUDA kernel
    wav_list = [librosa.util.fix_length(w, 16000*30) for w in wav_list]
    inputs = self.processor(
        wav_list, sampling_rate=16_000,
        return_tensors="pt", padding=False).to(DEVICE)   # æ•´æ‰¹æ”¾ GPU
    with torch.no_grad():
        hidden = self.backbone(**inputs).last_hidden_state        # BÃ—TÃ—768
    emb = self.pool(hidden.transpose(1, 2)).squeeze(-1)          # BÃ—768
    return emb.cpu().numpy()

# æ”¾åœ¨ load_data_from_wav_files_optimized ä¹‹å
def load_uci_updrs(csv_path):
    """
    ä»…è¿”å› 43-D ç‰¹å¾ + motor_UPDRS
    ç‰¹å¾é¡ºåºä¸ extract_audio_features ä¿æŒä¸€è‡´å³å¯
    """
    df = pd.read_csv(csv_path)
    # æ‰‹å·¥ç‰¹å¾åˆ—ï¼ˆä¸ wav ä¾§ 43-D å¯¹åº”ï¼‰
    feat_cols = ['Jitter(%)','Jitter(Abs)','Jitter:RAP','Jitter:PPQ5','Jitter:DDP',
                 'Shimmer','Shimmer(dB)','Shimmer:APQ3','Shimmer:APQ5','Shimmer:APQ11','Shimmer:DDA',
                 'NHR','HNR','RPDE','DFA','PPE',
                 'f0_mean','f0_std','f0_range',          # ä¸‹é¢ 3 åˆ—éœ€è¦ä½ è‡ªå·±æ˜ å°„
                 'spectral_centroid_mean','spectral_centroid_std',
                 'spectral_bandwidth_mean','spectral_bandwidth_std',
                 'spectral_rolloff_mean','spectral_rolloff_std',
                 'zcr_mean','zcr_std',
                 'rms_mean','rms_std',
                 'shimmer','jitter',
                 'kurtosis','skewness',
                 'mfcc_1_mean','mfcc_1_std','mfcc_2_mean','mfcc_2_std',
                 'mfcc_3_mean','mfcc_3_std','mfcc_4_mean','mfcc_4_std',
                 'mfcc_5_mean','mfcc_5_std','mfcc_6_mean','mfcc_6_std',
                 'mfcc_7_mean','mfcc_7_std','mfcc_8_mean','mfcc_8_std']
    # ç¼ºå•¥è¡¥ 0ï¼›è¿™é‡Œåªåˆ—å‡º UCI æœ‰çš„ï¼Œå…± 22 åˆ—ï¼Œå…¶ä½™è¡¥ 0 â†’ 43-D
    uci_avail = ['Jitter(%)','Jitter:A','Jitter:RAP','Jitter:PPQ5','Jitter:DDP',
                 'Shimmer','Shimmer(dB)','Shimmer:APQ3','Shimmer:APQ5','Shimmer:APQ11','Shimmer:DDA',
                 'NHR','HNR','RPDE','DFA','PPE']
    X = df[uci_avail].values
    # è¡¥åˆ° 43-D
    pad = np.zeros((X.shape[0], 43 - X.shape[1]))
    X = np.hstack([X, pad])
    y_updrs = df['motor_UPDRS'].values.astype(np.float32)
    return X, y_updrs


class SeverityTrainer:
    def __init__(self, config, encoder_model):
        self.config = config
        self.encoder = encoder_model          # å†»ç»“çš„ encoderï¼ˆæ¥è‡ª wav è®­ç»ƒï¼‰
        self.scaler = StandardScaler()
        self.results = {}

    def build_severity_head(self):
        return tf.keras.Sequential([
            layers.Dense(64, activation='relu'),
            layers.Dense(1)   # é¢„æµ‹ motor_UPDRS
        ], name='severity_head')

    def train_eval(self, X_uci, y_uci, n_fold=5):
        kf = KFold(n_splits=n_fold, shuffle=True, random_state=self.config['RANDOM_STATE'])
        all_true, all_pred = [], []
        for fold, (tr, va) in enumerate(kf.split(X_uci)):
            # æ ‡å‡†åŒ–
            X_tr = self.scaler.fit_transform(X_uci[tr])
            X_va = self.scaler.transform(X_uci[va])
            y_tr, y_va = y_uci[tr], y_uci[va]

            # å†»ç»“ encoder + æ–°å»ºå¤´
            head = self.build_severity_head()
            inputs = layers.Input(shape=(self.config['EMBEDDING_DIM'],))
            out = head(inputs)
            model = Model(inputs, out)
            model.compile(optimizer=AdamW(learning_rate=1e-4), loss='mse', metrics=['mae'])

            # ç”ŸæˆåµŒå…¥ï¼ˆåªè·‘ä¸€éï¼Œçœ GPUï¼‰
            emb_tr = self.encoder({'handcrafted': tf.constant(X_tr, dtype=tf.float32),
                                   'raw_audio':  tf.zeros((len(X_tr), 480000))}, training=False)
            emb_va = self.encoder({'handcrafted': tf.constant(X_va, dtype=tf.float32),
                                   'raw_audio':  tf.zeros((len(X_va), 480000))}, training=False)

            # è®­ç»ƒ severity å¤´
            cb = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)
            model.fit(emb_tr, y_tr, validation_data=(emb_va, y_va),
                      epochs=60, batch_size=32, callbacks=[cb], verbose=0)

            pred = model.predict(emb_va, verbose=0).flatten()
            all_true.extend(y_va)
            all_pred.extend(pred)

        self.results = {'true': np.array(all_true),
                        'pred': np.array(all_pred),
                        'r':  np.corrcoef(all_true, all_pred)[0,1],
                        'mae': np.mean(np.abs(np.array(all_true) - np.array(all_pred)))}
        return self.results

# --- åˆ›æ–°ç‚¹1: å¤šæ¨¡æ€äº¤å‰æ³¨æ„åŠ›èåˆæœºåˆ¶ ---
# --- ä¿®å¤åçš„å¤šæ¨¡æ€äº¤å‰æ³¨æ„åŠ›èåˆæœºåˆ¶ ---
class MultiModalCrossAttentionFusion(layers.Layer):
    """
    å¤šæ¨¡æ€äº¤å‰æ³¨æ„åŠ›èåˆå±‚
    ä½¿ç”¨æ·±åº¦ç‰¹å¾ä½œä¸ºQueryï¼Œä¼ ç»Ÿå£°å­¦ç‰¹å¾ä½œä¸ºKeyå’ŒValue
    """

    def __init__(self, d_model=512, num_heads=8, dropout=0.1, **kwargs):
        super().__init__(**kwargs)
        self.d_model = d_model
        self.num_heads = num_heads
        self.dropout_rate = dropout

        # ç‰¹å¾æŠ•å½±å±‚
        self.deep_projection = layers.Dense(d_model, name='deep_projection')
        self.handcrafted_projection = layers.Dense(d_model, name='handcrafted_projection')

        # å¤šå¤´äº¤å‰æ³¨æ„åŠ› - ä¿®å¤ç»´åº¦é—®é¢˜
        self.cross_attention = layers.MultiHeadAttention(
            num_heads=num_heads,
            key_dim=d_model // num_heads,
            dropout=dropout,
            name='cross_attention'
        )

        # å‰é¦ˆç½‘ç»œ
        self.ffn = tf.keras.Sequential([
            layers.Dense(d_model * 4, activation='relu'),
            layers.Dropout(dropout),
            layers.Dense(d_model)
        ], name='ffn')

        # å±‚å½’ä¸€åŒ–
        self.ln1 = layers.LayerNormalization(name='ln1')
        self.ln2 = layers.LayerNormalization(name='ln2')
        self.dropout = layers.Dropout(dropout)

    def build(self, input_shape):
        """æ„å»ºå±‚"""
        super().build(input_shape)

    def call(self, inputs, training=False, return_attention_scores=False):
        """
        inputs: [deep_features, handcrafted_features]
        """
        deep_features, handcrafted_features = inputs

        # ç¡®ä¿è¾“å…¥ç»´åº¦æ­£ç¡®
        if len(tf.shape(deep_features)) == 1:
            deep_features = tf.expand_dims(deep_features, 0)
        if len(tf.shape(handcrafted_features)) == 1:
            handcrafted_features = tf.expand_dims(handcrafted_features, 0)

        # æŠ•å½±åˆ°ç›¸åŒç»´åº¦
        deep_proj = self.deep_projection(deep_features)  # (B, d_model)
        handcrafted_proj = self.handcrafted_projection(handcrafted_features)  # (B, d_model)

        # ä¸ºäº¤å‰æ³¨æ„åŠ›æ·»åŠ åºåˆ—ç»´åº¦
        query = tf.expand_dims(deep_proj, axis=1)  # (B, 1, d_model)
        key = tf.expand_dims(handcrafted_proj, axis=1)  # (B, 1, d_model)
        value = tf.expand_dims(handcrafted_proj, axis=1)  # (B, 1, d_model)

        # äº¤å‰æ³¨æ„åŠ›ï¼šæ·±åº¦ç‰¹å¾æŸ¥è¯¢ä¼ ç»Ÿç‰¹å¾
        attention_output = self.cross_attention(
            query=query,
            key=key,
            value=value,
            training=training,
            return_attention_scores=return_attention_scores
        )

        # å¤„ç†è¿”å›å€¼
        if return_attention_scores:
            attention_output, attention_scores = attention_output
            # ç§»é™¤åºåˆ—ç»´åº¦
            attention_output = tf.squeeze(attention_output, axis=1)  # (B, d_model)
            # å¤„ç†æ³¨æ„åŠ›åˆ†æ•°ç»´åº¦
            attention_scores = tf.reduce_mean(attention_scores, axis=[1, 2])  # (B, num_heads)
        else:
            # ç§»é™¤åºåˆ—ç»´åº¦
            attention_output = tf.squeeze(attention_output, axis=1)  # (B, d_model)

        # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
        x = self.ln1(deep_proj + self.dropout(attention_output, training=training))

        # å‰é¦ˆç½‘ç»œ
        ffn_output = self.ffn(x, training=training)
        output = self.ln2(x + self.dropout(ffn_output, training=training))

        if return_attention_scores:
            return output, attention_scores
        return output


# --- åˆ›æ–°ç‚¹2: ä¸ç¡®å®šæ€§åŠ æƒå¤šä»»åŠ¡æŸå¤± ---
# --- ä¿®å¤åçš„ä¸ç¡®å®šæ€§åŠ æƒå¤šä»»åŠ¡æŸå¤± ---
class UncertaintyWeightedLoss(layers.Layer):
    """
    ä¸ç¡®å®šæ€§åŠ æƒå¤šä»»åŠ¡æŸå¤±
    åŸºäº Kendall et al. "Multi-Task Learning Using Uncertainty to Weigh Losses" (CVPR 2018)
    """

    def __init__(self, num_tasks=3, **kwargs):
        super().__init__(**kwargs)
        self.num_tasks = num_tasks

        # å­¦ä¹ æ¯ä¸ªä»»åŠ¡çš„ä¸ç¡®å®šæ€§å‚æ•° (log variance)
        self.log_vars = self.add_weight(
            name='log_vars',
            shape=(num_tasks,),
            initializer='zeros',
            trainable=True
        )

    def build(self, input_shape):
        """æ„å»ºå±‚"""
        super().build(input_shape)

    def call(self, losses):
        """
        losses: [classification_loss, contrastive_loss, clinical_loss]
        """
        weighted_losses = []
        precision_losses = []

        for i, loss in enumerate(losses):
            # ç¡®ä¿lossæ˜¯å¼ é‡
            loss_tensor = tf.convert_to_tensor(loss, dtype=tf.float32)

            # è®¡ç®—ç²¾åº¦ (1/variance)
            precision = tf.exp(-self.log_vars[i])

            # åŠ æƒæŸå¤±
            weighted_loss = precision * loss_tensor
            weighted_losses.append(weighted_loss)

            # æ­£åˆ™åŒ–é¡¹ (log variance)
            precision_losses.append(self.log_vars[i])

        # æ€»æŸå¤± = åŠ æƒæŸå¤±ä¹‹å’Œ + æ­£åˆ™åŒ–é¡¹
        total_weighted_loss = tf.add_n(weighted_losses)
        regularization = tf.add_n(precision_losses)

        return total_weighted_loss + regularization, weighted_losses, precision_losses


# --- Wav2Vecç‰¹å¾æå–å™¨ ---
if TORCH_AVAILABLE:
    class Wav2VecEncoder:
        def __init__(self, model_name="E:/wav2vec2-base-960h"):
            self.processor = Wav2Vec2Processor.from_pretrained(model_name)
            self.backbone = Wav2Vec2Model.from_pretrained(model_name).to(DEVICE)
            self.backbone.eval()
            self.pool = nn.AdaptiveAvgPool1d(1)

        def extract_features(self, wav_16k):
            """æå–Wav2Vecç‰¹å¾"""
            inputs = self.processor(wav_16k, sampling_rate=16000, return_tensors="pt", padding=True).to(DEVICE)
            with torch.no_grad():
                hidden = self.backbone(**inputs).last_hidden_state
            hidden = hidden.transpose(1, 2)
            emb = self.pool(hidden).squeeze(-1)
            return emb.cpu().numpy()


# --- å¤šæ¨¡æ€ç¼–ç å™¨ ---
# --- ä¿®å¤åçš„å¤šæ¨¡æ€ç¼–ç å™¨ ---
class MultiModalEncoder(Model):
    """
    å¤šæ¨¡æ€ç¼–ç å™¨ï¼Œç»“åˆä¼ ç»Ÿå£°å­¦ç‰¹å¾å’Œæ·±åº¦ç‰¹å¾
    """

    def __init__(self, handcrafted_dim=40, deep_dim=768, embedding_dim=512,
                 num_heads=8, dropout=0.3, **kwargs):
        super().__init__(**kwargs)
        self.handcrafted_dim = handcrafted_dim
        self.deep_dim = deep_dim
        self.embedding_dim = embedding_dim

        # Wav2Vecç‰¹å¾æå–å™¨
        if TORCH_AVAILABLE:
            self.wav2vec = Wav2VecEncoder()

        # å¤šæ¨¡æ€äº¤å‰æ³¨æ„åŠ›èåˆ
        self.mm_caf = MultiModalCrossAttentionFusion(
            d_model=embedding_dim,
            num_heads=num_heads,
            dropout=dropout
        )

        # æœ€ç»ˆæŠ•å½±å±‚
        self.final_projection = tf.keras.Sequential([
            layers.Dense(embedding_dim, activation='relu'),
            layers.Dropout(dropout),
            layers.Dense(embedding_dim)
        ])

    def build(self, input_shape):
        """æ„å»ºæ¨¡å‹"""
        super().build(input_shape)

    def call(self, inputs, training=False, return_attention_scores=False):
        """
        inputs: {'handcrafted': (B, handcrafted_dim), 'raw_audio': (B, T)}
        """
        handcrafted_features = inputs['handcrafted']

        # æå–æ·±åº¦ç‰¹å¾
        if TORCH_AVAILABLE and 'raw_audio' in inputs:
            try:
                raw_audio = inputs['raw_audio'].numpy()
                deep_features_list = []

                # æ‰¹é‡å¤„ç†éŸ³é¢‘ç‰¹å¾æå–
                batch_size = len(raw_audio)
                for i in range(batch_size):
                    try:
                        audio = raw_audio[i]
                        # ç¡®ä¿éŸ³é¢‘é•¿åº¦åˆé€‚ - å‡å°‘å¤„ç†æ—¶é—´
                        if len(audio) > 48000:  # 3ç§’@16kHz
                            audio = audio[:48000]
                        elif len(audio) < 16000:  # æœ€å°1ç§’
                            audio = np.pad(audio, (0, 16000 - len(audio)), mode='constant')

                        feat = self.wav2vec.extract_features(audio)
                        if len(feat.shape) == 1:
                            feat = feat.reshape(1, -1)
                        deep_features_list.append(feat[0])
                    except Exception as e:
                        print(f"Warning: Audio {i} processing failed: {e}")
                        # ä½¿ç”¨é›¶å‘é‡ä½œä¸ºåå¤‡
                        deep_features_list.append(np.zeros(self.deep_dim))

                deep_features = tf.constant(np.array(deep_features_list), dtype=tf.float32)
            except Exception as e:
                print(f"Warning: Wav2Vec feature extraction failed: {e}")
                # ä½¿ç”¨éšæœºç‰¹å¾ä½œä¸ºåå¤‡
                batch_size = tf.shape(handcrafted_features)[0]
                deep_features = tf.random.normal((batch_size, self.deep_dim))
        else:
            # å¦‚æœæ²¡æœ‰åŸå§‹éŸ³é¢‘ï¼Œä½¿ç”¨éšæœºæ·±åº¦ç‰¹å¾ä½œä¸ºå ä½ç¬¦
            batch_size = tf.shape(handcrafted_features)[0]
            deep_features = tf.random.normal((batch_size, self.deep_dim))

        # å¤šæ¨¡æ€äº¤å‰æ³¨æ„åŠ›èåˆ
        try:
            if return_attention_scores:
                fused_features, attention_scores = self.mm_caf(
                    [deep_features, handcrafted_features],
                    training=training,
                    return_attention_scores=True
                )
            else:
                fused_features = self.mm_caf(
                    [deep_features, handcrafted_features],
                    training=training
                )
        except Exception as e:
            print(f"Warning: Cross-attention fusion failed: {e}")
            # ç®€å•æ‹¼æ¥ä½œä¸ºåå¤‡
            try:
                deep_proj = layers.Dense(self.embedding_dim)(deep_features)
                hand_proj = layers.Dense(self.embedding_dim)(handcrafted_features)
                fused_features = (deep_proj + hand_proj) / 2
            except Exception as e2:
                print(f"Warning: Fallback fusion also failed: {e2}")
                # æœ€ç®€å•çš„æ‹¼æ¥
                fused_features = tf.concat([deep_features, handcrafted_features], axis=-1)
                fused_features = layers.Dense(self.embedding_dim)(fused_features)

            if return_attention_scores:
                attention_scores = tf.zeros((tf.shape(handcrafted_features)[0], 8))

        # æœ€ç»ˆæŠ•å½±
        output = self.final_projection(fused_features, training=training)

        if return_attention_scores:
            return output, attention_scores
        return output


# --- ProtoNet with Uncertainty Weighting ---
# --- ä¿®å¤åçš„å¢å¼ºProtoNet ---
class EnhancedProtoNet(Model):
    """
    å¢å¼ºçš„åŸå‹ç½‘ç»œï¼Œé›†æˆå¤šæ¨¡æ€äº¤å‰æ³¨æ„åŠ›å’Œä¸ç¡®å®šæ€§åŠ æƒæŸå¤±
    """

    def __init__(self, num_classes=2, embedding_dim=512, temperature=1.0,
                 use_uncertainty_weighting=True, **kwargs):
        super().__init__(**kwargs)
        self.num_classes = num_classes
        self.embedding_dim = embedding_dim
        self.use_uncertainty_weighting = use_uncertainty_weighting

        # å¤šæ¨¡æ€ç¼–ç å™¨
        self.encoder = MultiModalEncoder(embedding_dim=embedding_dim)

        # æ¸©åº¦å‚æ•°
        self.temperature = self.add_weight(
            name='temperature',
            shape=(),
            initializer=tf.constant_initializer(temperature),
            trainable=True
        )

        # ä¸ç¡®å®šæ€§åŠ æƒæŸå¤±
        if use_uncertainty_weighting:
            self.uncertainty_loss = UncertaintyWeightedLoss(num_tasks=3)

        # æŸå¤±å‡½æ•°
        self.ce_loss = SparseCategoricalCrossentropy(from_logits=True)

    def build(self, input_shape):
        """æ„å»ºæ¨¡å‹"""
        super().build(input_shape)

    def call(self, inputs, training=False, return_attention=False):
        """å‰å‘ä¼ æ’­"""
        if return_attention:
            embeddings, attention_scores = self.encoder(
                inputs, training=training, return_attention_scores=True
            )
            return embeddings, attention_scores
        else:
            embeddings = self.encoder(inputs, training=training)
            return embeddings

    def compute_prototypes(self, embeddings, labels):
        """è®¡ç®—ç±»åŸå‹"""
        prototypes = []
        for class_id in range(self.num_classes):
            mask = tf.equal(labels, class_id)
            class_embeddings = tf.boolean_mask(embeddings, mask)
            if tf.shape(class_embeddings)[0] > 0:
                prototype = tf.reduce_mean(class_embeddings, axis=0)
            else:
                prototype = tf.zeros(self.embedding_dim)
            prototypes.append(prototype)
        return tf.stack(prototypes)

    def compute_distances(self, embeddings, prototypes):
        """è®¡ç®—åˆ°åŸå‹çš„è·ç¦»"""
        # æ¬§å‡ é‡Œå¾—è·ç¦»
        distances = tf.norm(
            tf.expand_dims(embeddings, 1) - tf.expand_dims(prototypes, 0),
            axis=2
        )
        return -distances / self.temperature

    def supervised_contrastive_loss(self, embeddings, labels, temperature=0.1):
        """ç›‘ç£å¯¹æ¯”æŸå¤±"""
        # å½’ä¸€åŒ–åµŒå…¥
        embeddings = tf.nn.l2_normalize(embeddings, axis=1)

        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = tf.matmul(embeddings, embeddings, transpose_b=True) / temperature

        # åˆ›å»ºæ ‡ç­¾æ©ç 
        labels = tf.expand_dims(labels, 1)
        mask = tf.equal(labels, tf.transpose(labels))
        mask = tf.cast(mask, tf.float32)

        # ç§»é™¤å¯¹è§’çº¿
        logits_mask = tf.ones_like(mask) - tf.eye(tf.shape(mask)[0])
        mask = mask * logits_mask

        # è®¡ç®—å¯¹æ¯”æŸå¤±
        exp_logits = tf.exp(similarity_matrix) * logits_mask
        log_prob = similarity_matrix - tf.math.log(tf.reduce_sum(exp_logits, axis=1, keepdims=True) + 1e-8)

        # é¿å…é™¤é›¶
        mask_sum = tf.reduce_sum(mask, axis=1)
        mask_sum = tf.where(mask_sum > 0, mask_sum, tf.ones_like(mask_sum))

        mean_log_prob_pos = tf.reduce_sum(mask * log_prob, axis=1) / mask_sum
        loss = -tf.reduce_mean(mean_log_prob_pos)

        return loss

    def clinical_center_loss(self, embeddings, labels, updrs_scores, temperature=0.1):
        """ä¸´åºŠä¸­å¿ƒæŸå¤±"""
        embeddings = tf.cast(embeddings, tf.float32)
        labels = tf.cast(labels, tf.int32)
        updrs_scores = tf.cast(updrs_scores, tf.float32)

        # è®¡ç®—åŸå‹
        prototypes = tf.math.unsorted_segment_mean(embeddings, labels, num_segments=2)

        # ä»…å¯¹PDç±»è€ƒè™‘ä¸¥é‡åº¦
        mask_pd = tf.cast(labels, tf.bool)
        emb_pd = tf.boolean_mask(embeddings, mask_pd)
        score_pd = tf.boolean_mask(updrs_scores, mask_pd)

        if tf.shape(emb_pd)[0] > 0:
            # è®¡ç®—åˆ°åŸå‹è·ç¦»
            dist = tf.norm(emb_pd - prototypes[1], axis=1)
            # å¸Œæœ›è·ç¦»ä¸ä¸¥é‡åº¦æˆåæ¯”
            max_score = tf.reduce_max(score_pd)
            max_score = tf.where(max_score > 0, max_score, tf.ones_like(max_score))
            target = 1.0 - (score_pd / max_score)
            loss = tf.reduce_mean(tf.square(dist - target))
        else:
            loss = tf.constant(0.0, dtype=tf.float32)

        return loss

    def compute_loss(self, inputs, labels, updrs_scores=None):
        """è®¡ç®—æ€»æŸå¤±"""
        embeddings = self(inputs, training=True)

        # è®¡ç®—åŸå‹å’Œè·ç¦»
        prototypes = self.compute_prototypes(embeddings, labels)
        logits = self.compute_distances(embeddings, prototypes)

        # åˆ†ç±»æŸå¤±
        ce_loss = self.ce_loss(labels, logits)

        # å¯¹æ¯”æŸå¤±
        contrastive_loss = self.supervised_contrastive_loss(embeddings, labels)

        # ä¸´åºŠæŸå¤±
        if updrs_scores is not None and self.config.get('USE_CLINICAL_CENTER_LOSS', False):
            clinical_loss = self.clinical_center_loss(embeddings, labels, updrs_scores)
        else:
            clinical_loss = tf.constant(0.0, dtype=tf.float32)

        # ç¡®ä¿æ‰€æœ‰æŸå¤±éƒ½æ˜¯å¼ é‡
        losses = [
            tf.convert_to_tensor(ce_loss, dtype=tf.float32),
            tf.convert_to_tensor(contrastive_loss, dtype=tf.float32),
            tf.convert_to_tensor(clinical_loss, dtype=tf.float32)
        ]

        # ä¸ç¡®å®šæ€§åŠ æƒ
        if self.use_uncertainty_weighting:
            total_loss, weighted_losses, precision_losses = self.uncertainty_loss(losses)
            return {
                'total_loss': total_loss,
                'ce_loss': ce_loss,
                'contrastive_loss': contrastive_loss,
                'clinical_loss': clinical_loss,
                'weighted_losses': weighted_losses,
                'precision_losses': precision_losses,
                'logits': logits
            }
        else:
            # ä¼ ç»ŸåŠ æƒ
            total_loss = losses[0] + 0.5 * losses[1] + 0.5 * losses[2]
            return {
                'total_loss': total_loss,
                'ce_loss': ce_loss,
                'contrastive_loss': contrastive_loss,
                'clinical_loss': clinical_loss,
                'logits': logits
            }


# --- ç‰¹å¾æå–å‡½æ•° ---
def extract_audio_features(file_path, sr=16000, fast_mode=True):
    """ä¼˜åŒ–çš„éŸ³é¢‘ç‰¹å¾æå–å‡½æ•°"""
    try:
        # å¿«é€Ÿæ¨¡å¼ï¼šåªåŠ è½½å‰3ç§’
        if fast_mode:
            y, _ = librosa.load(file_path, sr=sr, duration=3.0)
        else:
            y, _ = librosa.load(file_path, sr=sr)

        if len(y) == 0:
            return None, None

        # åŸºç¡€ç‰¹å¾å­—å…¸
        features = {}

        # 1. åŸºé¢‘ç‰¹å¾ (F0) - ä½¿ç”¨æ›´ç¨³å®šçš„æ–¹æ³•
        try:
            f0, voiced_flag, voiced_probs = librosa.pyin(
                y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7')
            )
            f0_clean = f0[~np.isnan(f0)]

            if len(f0_clean) > 0:
                features['f0_mean'] = np.mean(f0_clean)
                features['f0_std'] = np.std(f0_clean)
                features['f0_range'] = np.ptp(f0_clean)
                features['f0_median'] = np.median(f0_clean)
            else:
                features['f0_mean'] = features['f0_std'] = features['f0_range'] = features['f0_median'] = 0
        except Exception as e:
            print(f"F0 extraction failed: {e}")
            features['f0_mean'] = features['f0_std'] = features['f0_range'] = features['f0_median'] = 0

        # 2. MFCCç‰¹å¾
        try:
            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
            for i in range(13):
                features[f'mfcc_{i + 1}_mean'] = np.mean(mfccs[i])
                features[f'mfcc_{i + 1}_std'] = np.std(mfccs[i])
        except Exception as e:
            print(f"MFCC extraction failed: {e}")
            for i in range(13):
                features[f'mfcc_{i + 1}_mean'] = 0
                features[f'mfcc_{i + 1}_std'] = 0

        # 3. é¢‘è°±ç‰¹å¾
        try:
            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
            features['spectral_centroid_mean'] = np.mean(spectral_centroids)
            features['spectral_centroid_std'] = np.std(spectral_centroids)

            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]
            features['spectral_rolloff_mean'] = np.mean(spectral_rolloff)
            features['spectral_rolloff_std'] = np.std(spectral_rolloff)

            zero_crossing_rate = librosa.feature.zero_crossing_rate(y)[0]
            features['zcr_mean'] = np.mean(zero_crossing_rate)
            features['zcr_std'] = np.std(zero_crossing_rate)
        except Exception as e:
            print(f"Spectral features extraction failed: {e}")
            features['spectral_centroid_mean'] = features['spectral_centroid_std'] = 0
            features['spectral_rolloff_mean'] = features['spectral_rolloff_std'] = 0
            features['zcr_mean'] = features['zcr_std'] = 0

        # 4. è‰²åº¦ç‰¹å¾
        try:
            chroma = librosa.feature.chroma_stft(y=y, sr=sr)
            features['chroma_mean'] = np.mean(chroma)
            features['chroma_std'] = np.std(chroma)
        except Exception as e:
            print(f"Chroma extraction failed: {e}")
            features['chroma_mean'] = features['chroma_std'] = 0

        # 5. æ—¶åŸŸç»Ÿè®¡ç‰¹å¾
        try:
            features['rms_energy'] = np.sqrt(np.mean(y ** 2))
            features['kurtosis'] = kurtosis(y)
            features['skewness'] = skew(y)
            features['signal_mean'] = np.mean(y)
            features['signal_std'] = np.std(y)
        except Exception as e:
            print(f"Time domain features extraction failed: {e}")
            features['rms_energy'] = features['kurtosis'] = features['skewness'] = 0
            features['signal_mean'] = features['signal_std'] = 0

        # 6. æŠ–åŠ¨å’Œå¾®é¢¤ç‰¹å¾ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        try:
            if len(f0_clean) > 1:
                jitter = np.std(np.diff(f0_clean)) / np.mean(f0_clean) if np.mean(f0_clean) > 0 else 0
                features['jitter'] = jitter
            else:
                features['jitter'] = 0
        except Exception as e:
            print(f"Jitter extraction failed: {e}")
            features['jitter'] = 0

        return features, y

    except Exception as e:
        print(f"Error processing {file_path}: {e}")
        return None, None


def load_data_from_wav_files_optimized(config):
    """
    ä»WAVæ–‡ä»¶åŠ è½½æ•°æ® - å¸¦ç¼“å­˜åŠŸèƒ½çš„ä¼˜åŒ–ç‰ˆæœ¬
    """
    print('Loading data from WAV files...')

    # æ•°æ®è·¯å¾„æ£€æŸ¥
    data_path = Path(config['DATA_PATH'])
    hc_dir = data_path / 'HC1'
    pd_dir = data_path / 'PD1'

    print(f'ğŸ“‚ æ•°æ®è·¯å¾„: {data_path}')
    print(f'ğŸ“‚ HC1è·¯å¾„: {hc_dir}')
    print(f'ğŸ“‚ PD1è·¯å¾„: {pd_dir}')

    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not hc_dir.exists():
        print(f'âŒ HC1ç›®å½•ä¸å­˜åœ¨: {hc_dir}')
        return None
    if not pd_dir.exists():
        print(f'âŒ PD1ç›®å½•ä¸å­˜åœ¨: {pd_dir}')
        return None

    # ç¼“å­˜æ–‡ä»¶è·¯å¾„
    cache_dir = data_path / 'cache'
    cache_dir.mkdir(exist_ok=True)

    features_cache_file = cache_dir / 'features_cache.pkl'
    audio_cache_file = cache_dir / 'audio_cache.pkl'

    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç¼“å­˜
    if features_cache_file.exists() and audio_cache_file.exists():
        try:
            print('ğŸ”„ å‘ç°ç¼“å­˜æ–‡ä»¶ï¼Œæ­£åœ¨åŠ è½½...')

            # åŠ è½½ç¼“å­˜çš„ç‰¹å¾
            with open(features_cache_file, 'rb') as f:
                cached_data = pickle.load(f)

            # åŠ è½½ç¼“å­˜çš„éŸ³é¢‘æ•°æ®
            with open(audio_cache_file, 'rb') as f:
                cached_audio = pickle.load(f)

            df = pd.DataFrame(cached_data['features'])
            df[config['TARGET_COLUMN']] = cached_data['labels']

            print(f'âœ… ä»ç¼“å­˜åŠ è½½ {len(df)} æ¡æ ·æœ¬ï¼Œåˆ†å¸ƒ:')
            print(df[config['TARGET_COLUMN']].value_counts())
            print(f'ç‰¹å¾ç»´åº¦: {len(df.columns) - 1}')

            # è¯¢é—®æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            use_cache = input('æ˜¯å¦ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼Ÿ(y/nï¼Œé»˜è®¤y): ').strip().lower()
            if use_cache in ['', 'y', 'yes']:
                return df, cached_audio['audio_data']
            else:
                print('ç”¨æˆ·é€‰æ‹©é‡æ–°æå–ç‰¹å¾...')

        except Exception as e:
            print(f'âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥: {e}ï¼Œå°†é‡æ–°æå–ç‰¹å¾')

    # é‡æ–°æå–ç‰¹å¾
    print('ğŸ” å¼€å§‹æå–ç‰¹å¾...')

    # æœç´¢WAVæ–‡ä»¶
    wav_files = list(hc_dir.rglob('*.wav')) + list(pd_dir.rglob('*.wav'))

    if not wav_files:
        print('âŒ æœªæ‰¾åˆ°ä»»ä½•WAVæ–‡ä»¶')
        print(f'è¯·æ£€æŸ¥ä»¥ä¸‹ç›®å½•ä¸­æ˜¯å¦åŒ…å«.wavæ–‡ä»¶:')
        print(f'  - {hc_dir}')
        print(f'  - {pd_dir}')
        return None

    print(f'ğŸ“ å…±å‘ç° {len(wav_files)} ä¸ª wav æ–‡ä»¶')

    # æ˜¾ç¤ºæ–‡ä»¶åˆ†å¸ƒ
    hc_files = [f for f in wav_files if 'HC1' in str(f)]
    pd_files = [f for f in wav_files if 'PD1' in str(f)]
    print(f'   - HC1: {len(hc_files)} ä¸ªæ–‡ä»¶')
    print(f'   - PD1: {len(pd_files)} ä¸ªæ–‡ä»¶')

    # å¯é€‰ï¼šé™åˆ¶æ–‡ä»¶æ•°é‡ç”¨äºå¿«é€Ÿè°ƒè¯•
    if config.get('MAX_FILES_FOR_DEBUG'):
        wav_files = wav_files[:config['MAX_FILES_FOR_DEBUG']]
        print(f'ğŸ“ è°ƒè¯•æ¨¡å¼ï¼šé™åˆ¶ä¸º {len(wav_files)} ä¸ªæ–‡ä»¶')

    all_features, all_labels, all_audio = [], [], []

    for file_path in tqdm(wav_files, desc='æå–ç‰¹å¾'):
        # æ ‡ç­¾é€»è¾‘ - åŸºäºè·¯å¾„åˆ¤æ–­
        if 'PD1' in str(file_path):
            label = 1  # å¸•é‡‘æ£®ç—…
        elif 'HC1' in str(file_path):
            label = 0  # å¥åº·å¯¹ç…§
        else:
            print(f'âš ï¸ æ— æ³•ç¡®å®šæ–‡ä»¶æ ‡ç­¾: {file_path}')
            continue

        features, audio = extract_audio_features(str(file_path), config['SAMPLE_RATE'])
        if features is not None:
            all_features.append(features)
            all_labels.append(label)
            all_audio.append(audio)

    if not all_features:
        print('âŒ æœªæå–åˆ°æœ‰æ•ˆç‰¹å¾')
        return None

    # ä¿å­˜åˆ°ç¼“å­˜
    try:
        print('ğŸ’¾ ä¿å­˜ç‰¹å¾åˆ°ç¼“å­˜...')

        # ä¿å­˜ç‰¹å¾ç¼“å­˜
        cache_data = {
            'features': all_features,
            'labels': all_labels,
            'timestamp': datetime.now().isoformat(),
            'config': dict(config),  # è½¬æ¢ä¸ºæ™®é€šå­—å…¸ä»¥ä¾¿åºåˆ—åŒ–
            'file_count': len(wav_files),
            'hc_count': len([l for l in all_labels if l == 0]),
            'pd_count': len([l for l in all_labels if l == 1])
        }
        with open(features_cache_file, 'wb') as f:
            pickle.dump(cache_data, f)

        # ä¿å­˜éŸ³é¢‘ç¼“å­˜
        audio_cache_data = {
            'audio_data': all_audio,
            'timestamp': datetime.now().isoformat()
        }
        with open(audio_cache_file, 'wb') as f:
            pickle.dump(audio_cache_data, f)

        print('âœ… ç¼“å­˜ä¿å­˜æˆåŠŸ')
        print(f'   - ç‰¹å¾ç¼“å­˜: {features_cache_file}')
        print(f'   - éŸ³é¢‘ç¼“å­˜: {audio_cache_file}')

    except Exception as e:
        print(f'âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}')

    df = pd.DataFrame(all_features)
    df[config['TARGET_COLUMN']] = all_labels
    print(f'âœ… æˆåŠŸåŠ è½½ {len(df)} æ¡æ ·æœ¬ï¼Œåˆ†å¸ƒ:')
    print(df[config['TARGET_COLUMN']].value_counts())
    print(f'ç‰¹å¾ç»´åº¦: {len(df.columns) - 1}')

    return df, all_audio

# --- è®­ç»ƒå’Œè¯„ä¼°å‡½æ•° ---
class EnhancedTrainer:
    """å¢å¼ºçš„è®­ç»ƒå™¨ï¼Œæ”¯æŒå¤šæ¨¡æ€å’Œä¸ç¡®å®šæ€§åŠ æƒ"""

    def __init__(self, config):
        self.config = config
        self.model = None
        self.scaler = StandardScaler()
        self.results = {}

    def prepare_data(self, df, audio_data):
        """ä¼˜åŒ–çš„æ•°æ®å‡†å¤‡æ–¹æ³•"""
        print("Preparing data for training...")

        # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
        feature_columns = [col for col in df.columns if col != self.config['TARGET_COLUMN']]
        X_handcrafted = df[feature_columns].values.astype(np.float32)
        y = df[self.config['TARGET_COLUMN']].values.astype(np.int32)

        # å¤„ç†ç¼ºå¤±å€¼
        imputer = SimpleImputer(strategy='median')
        X_handcrafted = imputer.fit_transform(X_handcrafted)

        # æ ‡å‡†åŒ–æ‰‹å·¥ç‰¹å¾
        if self.config['SCALE_DATA']:
            X_handcrafted = self.scaler.fit_transform(X_handcrafted)

        # éŸ³é¢‘æ•°æ®é¢„å¤„ç† - å¤§å¹…å‡å°‘ç›®æ ‡é•¿åº¦ä»¥åŠ é€Ÿå¤„ç†
        target_length = self.config['SAMPLE_RATE'] * 3  # æ”¹ä¸º3ç§’è€Œä¸æ˜¯30ç§’
        X_audio = np.zeros((len(audio_data), target_length), dtype=np.float32)

        print(f"Processing {len(audio_data)} audio files (target length: {target_length})...")

        for i, audio in enumerate(tqdm(audio_data, desc="Preparing audio")):
            if audio is None or len(audio) == 0:
                continue

            if len(audio) > target_length:
                # å–ä¸­é—´éƒ¨åˆ†è€Œä¸æ˜¯å¼€å¤´ï¼Œé€šå¸¸åŒ…å«æ›´å¤šä¿¡æ¯
                start_idx = (len(audio) - target_length) // 2
                X_audio[i] = audio[start_idx:start_idx + target_length]
            else:
                X_audio[i, :len(audio)] = audio

        print(f"Data preparation completed:")
        print(f"  Handcrafted features: {X_handcrafted.shape}")
        print(f"  Audio data: {X_audio.shape}")
        print(f"  Labels: {y.shape}")

        return X_handcrafted, X_audio, y

    def create_model(self):
        """åˆ›å»ºå¢å¼ºçš„ProtoNetæ¨¡å‹"""
        self.model = EnhancedProtoNet(
            num_classes=2,
            embedding_dim=self.config['EMBEDDING_DIM'],
            temperature=1.0,
            use_uncertainty_weighting=self.config['USE_UNCERTAINTY_WEIGHTING']
        )

        return self.model

    # --- ä¿®å¤åçš„è®­ç»ƒå™¨train_foldæ–¹æ³• ---
    def train_fold(self, X_handcrafted, X_audio, y, train_idx, val_idx, fold):
        """è®­ç»ƒå•ä¸ªæŠ˜"""
        print(f"\nTraining Fold {fold + 1}")

        # åˆ†å‰²æ•°æ®
        X_train_hc, X_val_hc = X_handcrafted[train_idx], X_handcrafted[val_idx]
        X_train_audio, X_val_audio = X_audio[train_idx], X_audio[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]

        # SMOTEå¤„ç†ï¼ˆä»…å¯¹ä¼ ç»Ÿç‰¹å¾ï¼‰
        if self.config['USE_SMOTE'] and SMOTE_AVAILABLE:
            try:
                # æ£€æŸ¥æœ€å°ç±»åˆ«æ ·æœ¬æ•°
                unique, counts = np.unique(y_train, return_counts=True)
                min_samples = min(counts)
                k_neighbors = min(self.config['SMOTE_K_NEIGHBORS'], min_samples - 1)

                if k_neighbors > 0:
                    smote = SMOTE(random_state=self.config['RANDOM_STATE'],
                                  k_neighbors=k_neighbors)
                    X_train_hc_smote, y_train_smote = smote.fit_resample(X_train_hc, y_train)

                    # ä¸ºSMOTEç”Ÿæˆçš„æ ·æœ¬åˆ›å»ºå¯¹åº”çš„éŸ³é¢‘æ•°æ®ï¼ˆä½¿ç”¨æœ€è¿‘é‚»ï¼‰
                    from sklearn.neighbors import NearestNeighbors
                    nn = NearestNeighbors(n_neighbors=1)
                    nn.fit(X_train_hc)
                    _, indices = nn.kneighbors(X_train_hc_smote[len(X_train_hc):])

                    X_train_audio_smote = np.vstack([
                        X_train_audio,
                        X_train_audio[indices.flatten()]
                    ])

                    X_train_hc, X_train_audio, y_train = X_train_hc_smote, X_train_audio_smote, y_train_smote
                    print(f"SMOTE applied: {len(y_train)} samples after augmentation")
                else:
                    print("SMOTE skipped: insufficient samples for k_neighbors")
            except Exception as e:
                print(f"SMOTE failed: {e}, continuing without SMOTE")

        # åˆ›å»ºæ¨¡å‹
        model = self.create_model()

        # ä¼˜åŒ–å™¨
        optimizer = AdamW(
            learning_rate=self.config['LEARNING_RATE'],
            weight_decay=self.config['WEIGHT_DECAY']
        )

        # è®­ç»ƒå¾ªç¯
        best_val_acc = 0
        patience_counter = 0
        train_losses = []
        val_accuracies = []

        for epoch in range(self.config['EPOCHS']):
            # è®­ç»ƒæ­¥éª¤
            epoch_losses = []

            # æ‰¹æ¬¡è®­ç»ƒ
            n_samples = len(X_train_hc)
            indices = np.random.permutation(n_samples)

            for start_idx in range(0, n_samples, self.config['BATCH_SIZE']):
                end_idx = min(start_idx + self.config['BATCH_SIZE'], n_samples)
                batch_indices = indices[start_idx:end_idx]

                batch_hc = X_train_hc[batch_indices]
                batch_audio = X_train_audio[batch_indices]
                batch_y = y_train[batch_indices]

                # å‡†å¤‡è¾“å…¥
                inputs = {
                    'handcrafted': tf.constant(batch_hc, dtype=tf.float32),
                    'raw_audio': tf.constant(batch_audio, dtype=tf.float32)
                }

                # è®¡ç®—æŸå¤±å’Œæ¢¯åº¦
                with tf.GradientTape() as tape:
                    try:
                        loss_dict = model.compute_loss(inputs, batch_y)
                        total_loss = loss_dict['total_loss']

                        # ç¡®ä¿æŸå¤±æ˜¯æ ‡é‡å¼ é‡
                        if tf.rank(total_loss) > 0:
                            total_loss = tf.reduce_mean(total_loss)

                    except Exception as e:
                        print(f"Warning: Loss computation failed: {e}")
                        # ä½¿ç”¨ç®€å•çš„åˆ†ç±»æŸå¤±ä½œä¸ºåå¤‡
                        try:
                            embeddings = model(inputs, training=True)
                            prototypes = model.compute_prototypes(embeddings, batch_y)
                            logits = model.compute_distances(embeddings, prototypes)
                            total_loss = model.ce_loss(batch_y, logits)
                        except Exception as e2:
                            print(f"Warning: Fallback loss also failed: {e2}")
                            continue

                # åå‘ä¼ æ’­
                try:
                    gradients = tape.gradient(total_loss, model.trainable_variables)
                    # è¿‡æ»¤Noneæ¢¯åº¦å¹¶è¿›è¡Œæ¢¯åº¦è£å‰ª
                    filtered_gradients = []
                    filtered_variables = []
                    for grad, var in zip(gradients, model.trainable_variables):
                        if grad is not None:
                            grad = tf.clip_by_norm(grad, 1.0)
                            filtered_gradients.append(grad)
                            filtered_variables.append(var)

                    if filtered_gradients:
                        optimizer.apply_gradients(zip(filtered_gradients, filtered_variables))
                        epoch_losses.append(float(total_loss.numpy()))

                except Exception as e:
                    print(f"Warning: Gradient update failed: {e}")
                    continue

            # éªŒè¯
            try:
                val_inputs = {
                    'handcrafted': tf.constant(X_val_hc, dtype=tf.float32),
                    'raw_audio': tf.constant(X_val_audio, dtype=tf.float32)
                }

                val_embeddings = model(val_inputs, training=False)
                val_prototypes = model.compute_prototypes(val_embeddings, y_val)
                val_logits = model.compute_distances(val_embeddings, val_prototypes)
                val_preds = tf.argmax(val_logits, axis=1).numpy()
                val_acc = accuracy_score(y_val, val_preds)
            except Exception as e:
                print(f"Warning: Validation failed: {e}")
                val_acc = 0.0

            if epoch_losses:
                train_losses.append(np.mean(epoch_losses))
            else:
                train_losses.append(0.0)
            val_accuracies.append(val_acc)

            # æ—©åœæ£€æŸ¥
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹æƒé‡
                try:
                    best_weights = model.get_weights()
                except:
                    best_weights = None
            else:
                patience_counter += 1

            if patience_counter >= self.config['PATIENCE']:
                print(f"Early stopping at epoch {epoch + 1}")
                break

            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch + 1}: Train Loss = {train_losses[-1]:.4f}, Val Acc = {val_acc:.4f}")

        # æ¢å¤æœ€ä½³æƒé‡
        if best_weights is not None:
            try:
                model.set_weights(best_weights)
            except:
                print("Warning: Failed to restore best weights")

        # æœ€ç»ˆè¯„ä¼°
        try:
            val_inputs = {
                'handcrafted': tf.constant(X_val_hc, dtype=tf.float32),
                'raw_audio': tf.constant(X_val_audio, dtype=tf.float32)
            }

            val_embeddings = model(val_inputs, training=False)
            val_prototypes = model.compute_prototypes(val_embeddings, y_val)
            val_logits = model.compute_distances(val_embeddings, val_prototypes)
            val_probs = tf.nn.softmax(val_logits).numpy()
            val_preds = tf.argmax(val_logits, axis=1).numpy()

            # è®¡ç®—æŒ‡æ ‡
            fold_results = {
                'accuracy': accuracy_score(y_val, val_preds),
                'balanced_accuracy': balanced_accuracy_score(y_val, val_preds),
                'roc_auc': roc_auc_score(y_val, val_probs[:, 1]) if len(np.unique(y_val)) > 1 else 0.5,
                'predictions': val_preds,
                'probabilities': val_probs,
                'true_labels': y_val,
                'train_losses': train_losses,
                'val_accuracies': val_accuracies
            }
        except Exception as e:
            print(f"Warning: Final evaluation failed: {e}")
            fold_results = {
                'accuracy': 0.0,
                'balanced_accuracy': 0.0,
                'roc_auc': 0.5,
                'predictions': np.zeros_like(y_val),
                'probabilities': np.random.rand(len(y_val), 2),
                'true_labels': y_val,
                'train_losses': train_losses,
                'val_accuracies': val_accuracies
            }

        return fold_results, model

    # # å°†è¿™ä¸ªæ–¹æ³•æ›¿æ¢åˆ°EnhancedTrainerç±»ä¸­
    # EnhancedTrainer.train_fold = train_fold

    def cross_validate(self, X_handcrafted, X_audio, y):
        """æ‰§è¡Œäº¤å‰éªŒè¯"""
        skf = StratifiedKFold(n_splits=self.config['N_SPLITS'],
                              shuffle=True,
                              random_state=self.config['RANDOM_STATE'])

        fold_results = []
        all_predictions = []
        all_probabilities = []
        all_true_labels = []

        for fold, (train_idx, val_idx) in enumerate(skf.split(X_handcrafted, y)):
            try:
                fold_result, model = self.train_fold(X_handcrafted, X_audio, y, train_idx, val_idx, fold)
                fold_results.append(fold_result)

                all_predictions.extend(fold_result['predictions'])
                all_probabilities.extend(fold_result['probabilities'])
                all_true_labels.extend(fold_result['true_labels'])

                print(f"Fold {fold + 1} completed: Accuracy = {fold_result['accuracy']:.4f}")
            except Exception as e:
                print(f"Fold {fold + 1} failed: {e}")
                continue

        if not fold_results:
            print("All folds failed!")
            return None

        # æ±‡æ€»ç»“æœ
        self.results = {
            'fold_results': fold_results,
            'mean_accuracy': np.mean([r['accuracy'] for r in fold_results]),
            'std_accuracy': np.std([r['accuracy'] for r in fold_results]),
            'mean_balanced_accuracy': np.mean([r['balanced_accuracy'] for r in fold_results]),
            'std_balanced_accuracy': np.std([r['balanced_accuracy'] for r in fold_results]),
            'mean_roc_auc': np.mean([r['roc_auc'] for r in fold_results]),
            'std_roc_auc': np.std([r['roc_auc'] for r in fold_results]),
            'all_predictions': np.array(all_predictions),
            'all_probabilities': np.array(all_probabilities),
            'all_true_labels': np.array(all_true_labels)
        }

        return self.results


def manage_cache(config):
    """
    ç¼“å­˜ç®¡ç†åŠŸèƒ½
    """
    data_path = Path(config['DATA_PATH'])
    cache_dir = data_path / 'cache'

    print(f'ğŸ“‚ ç¼“å­˜ç›®å½•: {cache_dir}')

    if not cache_dir.exists():
        print('ğŸ“‚ ç¼“å­˜ç›®å½•ä¸å­˜åœ¨ï¼Œå°†åœ¨é¦–æ¬¡è¿è¡Œæ—¶åˆ›å»º')
        return True  # ç»§ç»­æ‰§è¡Œï¼Œè®©ç¨‹åºåˆ›å»ºç¼“å­˜

    features_cache_file = cache_dir / 'features_cache.pkl'
    audio_cache_file = cache_dir / 'audio_cache.pkl'

    print('\n=== ç¼“å­˜ç®¡ç† ===')

    # æ˜¾ç¤ºç¼“å­˜ä¿¡æ¯
    if features_cache_file.exists():
        try:
            with open(features_cache_file, 'rb') as f:
                cached_data = pickle.load(f)

            print(f'ğŸ“Š ç‰¹å¾ç¼“å­˜ä¿¡æ¯:')
            print(f'   - æ ·æœ¬æ•°é‡: {len(cached_data["features"])}')
            print(f'   - HCæ ·æœ¬: {cached_data.get("hc_count", "æœªçŸ¥")}')
            print(f'   - PDæ ·æœ¬: {cached_data.get("pd_count", "æœªçŸ¥")}')
            print(f'   - åˆ›å»ºæ—¶é—´: {cached_data.get("timestamp", "æœªçŸ¥")}')
            print(f'   - æ–‡ä»¶å¤§å°: {features_cache_file.stat().st_size / 1024 / 1024:.2f} MB')

            if audio_cache_file.exists():
                print(f'   - éŸ³é¢‘ç¼“å­˜å¤§å°: {audio_cache_file.stat().st_size / 1024 / 1024:.2f} MB')

        except Exception as e:
            print(f'âŒ ç¼“å­˜ä¿¡æ¯è¯»å–å¤±è´¥: {e}')
    else:
        print('ğŸ“‚ æœªæ‰¾åˆ°ç‰¹å¾ç¼“å­˜æ–‡ä»¶')

    # ç¼“å­˜æ“ä½œé€‰é¡¹
    print('\nç¼“å­˜æ“ä½œé€‰é¡¹:')
    print('1. ä½¿ç”¨ç°æœ‰ç¼“å­˜ (æ¨è)')
    print('2. æ¸…é™¤ç¼“å­˜å¹¶é‡æ–°æå–')
    print('3. æŸ¥çœ‹ç¼“å­˜è¯¦ç»†ä¿¡æ¯')
    print('4. ç»§ç»­ä½¿ç”¨ç¼“å­˜')

    choice = input('è¯·é€‰æ‹©æ“ä½œ (1-4, é»˜è®¤1): ').strip()

    if choice == '2':
        # æ¸…é™¤ç¼“å­˜
        try:
            if features_cache_file.exists():
                features_cache_file.unlink()
                print(f'ğŸ—‘ï¸ å·²åˆ é™¤ç‰¹å¾ç¼“å­˜: {features_cache_file}')
            if audio_cache_file.exists():
                audio_cache_file.unlink()
                print(f'ğŸ—‘ï¸ å·²åˆ é™¤éŸ³é¢‘ç¼“å­˜: {audio_cache_file}')
            print('âœ… ç¼“å­˜å·²æ¸…é™¤ï¼Œå°†é‡æ–°æå–ç‰¹å¾')
            return False  # ä¸ä½¿ç”¨ç¼“å­˜
        except Exception as e:
            print(f'âŒ ç¼“å­˜æ¸…é™¤å¤±è´¥: {e}')
            return False

    elif choice == '3':
        # æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯
        if features_cache_file.exists():
            try:
                with open(features_cache_file, 'rb') as f:
                    cached_data = pickle.load(f)

                print(f'\nğŸ“‹ ç¼“å­˜è¯¦ç»†ä¿¡æ¯:')
                print(f'   - ç‰¹å¾æ•°é‡: {len(cached_data["features"])}')
                print(f'   - æ ‡ç­¾åˆ†å¸ƒ: {pd.Series(cached_data["labels"]).value_counts().to_dict()}')

                # æ˜¾ç¤ºç‰¹å¾åç§°
                if cached_data["features"]:
                    feature_names = list(cached_data["features"][0].keys())
                    print(f'   - ç‰¹å¾ç»´åº¦: {len(feature_names)}')
                    print(f'   - å‰10ä¸ªç‰¹å¾: {feature_names[:10]}')

            except Exception as e:
                print(f'âŒ è¯¦ç»†ä¿¡æ¯è¯»å–å¤±è´¥: {e}')

        return True  # ä½¿ç”¨ç¼“å­˜

    else:
        # é»˜è®¤ä½¿ç”¨ç¼“å­˜
        return True


# --- ä¸»å‡½æ•° ---
def main():
    """ä¸»å‡½æ•°"""
    print("Enhanced Parkinson's Disease Voice Analysis with Multi-Modal Cross-Attention Fusion")
    print("=" * 80)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(CONFIG['OUTPUT_PATH'])
    output_path.mkdir(exist_ok=True)

    try:
        """ä¸»å‡½æ•° - é›†æˆç¼“å­˜ç®¡ç†"""
        print("Enhanced Parkinson's Disease Voice Analysis with Multi-Modal Cross-Attention Fusion")
        print("=" * 80)

        # 1. ç¼“å­˜ç®¡ç†
        print("\n0. ç¼“å­˜ç®¡ç†...")
        use_cache = manage_cache(CONFIG)

        # 2. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
        print("\n1. Loading and preprocessing data...")

        if use_cache:
            # å°è¯•ä»ç¼“å­˜åŠ è½½
            result = load_data_from_wav_files_optimized(CONFIG)
        else:
            # å¼ºåˆ¶é‡æ–°æå–
            cache_dir = Path(CONFIG['DATA_PATH']) / 'cache'
            features_cache_file = cache_dir / 'features_cache.pkl'
            audio_cache_file = cache_dir / 'audio_cache.pkl'

            # ä¸´æ—¶åˆ é™¤ç¼“å­˜æ–‡ä»¶ä»¥å¼ºåˆ¶é‡æ–°æå–
            if features_cache_file.exists():
                features_cache_file.unlink()
            if audio_cache_file.exists():
                audio_cache_file.unlink()

            result = load_data_from_wav_files_optimized(CONFIG)

        if result is None:
            print("âŒ æ•°æ®åŠ è½½å¤±è´¥")
            return

        df, audio_data = result
        print(f"Loaded {len(df)} samples with {len(df.columns) - 1} features")

        # 3. å‡†å¤‡æ•°æ®
        print("\n2. Preparing data for training...")
        trainer = EnhancedTrainer(CONFIG)
        X_handcrafted, X_audio, y = trainer.prepare_data(df, audio_data)

        print(f"Handcrafted features shape: {X_handcrafted.shape}")
        print(f"Audio data shape: {X_audio.shape}")
        print(f"Labels shape: {y.shape}")

        # 4. è®­ç»ƒå’Œè¯„ä¼°
        print("\n3. Training and evaluating model...")
        results = trainer.cross_validate(X_handcrafted, X_audio, y)

        # 5. ç”Ÿæˆå¯è§†åŒ–
        print("\n5. Generating visualizations...")
        visualizer = SCIPaperVisualization(results, CONFIG['OUTPUT_PATH'])
        visualizer.create_all_figures()

        print("\nAnalysis completed successfully!")
        print(f"Results and visualizations saved to: {output_path}")

    except Exception as e:
        print(f"\nError during execution: {e}")
        traceback.print_exc()


if __name__ == "__main__":
    main()
